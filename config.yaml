# SpecialistHub Configuration Example
# Version: 3.0
#
# ==============================================================================
# DEVELOPER-LEVEL CONFIGURATION
# This file is the system's blueprint, intended for developers to define the
# agent's architecture. It should NOT be exposed to or editable by end-users.
# ==============================================================================
#
# This file defines the structure of the agentic system. It is the single
# source of truth for wiring together providers, models, and specialists.
# Copy this file to `config.yaml` and customize it for your setup.

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# Define the available LLM providers. The keys here (e.g., 'lmstudio') are
# referenced by the specialists below.
# The application will read environment variables defined in .env for secrets.
#
# Supported providers: "lmstudio", "ollama", "gemini"
# ==============================================================================
llm_providers:
  # A configuration for a local model via LM Studio
  lmstudio_router:
    # Uses the LMSTUDIO_BASE_URL from your .env file
    # Example: LMSTUDIO_BASE_URL=http://localhost:1234/v1
    type: "lmstudio"
    api_identifier: "lmstudio-community/gpt-oss-20b-gguf/router-gpt-oss-20b-mxfp4.gguf" # The model identifier for the server.
    # The model's total context window (input + output). This value is used by the llm
    # adapter to proactively prune the conversation history to prevent context overflow errors.
    context_window: 4096

  lmstudio_specialist:
    # Uses the LMSTUDIO_BASE_URL from your .env file
    # Example: LMSTUDIO_BASE_URL=http://localhost:1234/v1
    type: "lmstudio"
    api_identifier: "gpt-oss-20b" # The model identifier for the server.
    # The model's total context window (input + output).
    context_window: 8192
    # Parameters to pass to the model's API.
    parameters:
      # max_tokens controls the maximum length of the model's *output*.
      # It should be less than the context_window to leave room for the input prompt.
      max_tokens: 8192

  # A configuration for a local model via Ollama
  local_ollama:
    # Uses OLLAMA_BASE_URL and OLLAMA_MODEL from your .env file
    # Example: OLLAMA_BASE_URL=http://localhost:11434
    type: "ollama"
    api_identifier: "gemma3:27b" # The model name registered with Ollama.
    # Ollama uses different parameter names than OpenAI-compatible servers.
    parameters:
      # num_predict is Ollama's equivalent of max_tokens. It controls the
      # maximum number of tokens to generate in the response.
      num_predict: 4096

  # A configuration for a powerful, general-purpose Gemini model
  gemini_pro:
    # Uses GEMINI_API_KEY from your .env file
    type: "gemini"
    api_identifier: "gemini-2.5-pro"
    # Future parameters could go here, e.g., temperature: 0.5

  # A configuration for a fast, cheaper Gemini model
  gemini_flash:
    # Uses GEMINI_API_KEY from your .env file
    type: "gemini"
    api_identifier: "gemini-2.5-flash"

# ==============================================================================
# WORKFLOW CONFIGURATION
# ==============================================================================
# Define the entry point for the graph. Defaults to 'router_specialist'.
# ==============================================================================
workflow:
  entry_point: "prompt_triage_specialist"
  # The number of times a short sequence of specialists can repeat before the
  # workflow is halted to prevent unproductive loops.
  max_loop_cycles: 3
  # The maximum number of steps the graph can take before halting. This is a
  # final safeguard against infinite loops.
  recursion_limit: 40

# ==============================================================================
# SPECIALIST CONFIGURATION
# ==============================================================================
# Define each specialist agent. The key for each specialist (e.g., 'router_specialist')
# must match the specialist's Python module name (router_specialist.py) and the
# name passed to super().__init__() in its constructor.
# ==============================================================================
specialists:
  # --- Orchestration & Planning ---
  router_specialist:
    type: "llm"
    prompt_file: "router_prompt.md"
    description: "The master router and planner. It analyzes the user's request and routes it to the appropriate specialist. Its model should be capable of reliable tool/function calling."

  systems_architect:
    type: "llm"
    prompt_file: "systems_architect_prompt.md"
    description: "Analyzes a user's request and creates a high-level technical plan. Produces a 'system_plan' artifact. This is a good first step for complex tasks."

  critic_specialist:
    type: "llm"
    prompt_file: "critic_prompt.md"
    description: "Analyzes an HTML artifact and provides a critique for improvement. It is a key part of the refinement loop."

  # --- Core Capabilities ---
  web_builder:
    type: "llm"
    prompt_file: "web_builder_prompt.md"
    description: "Takes a 'system_plan' artifact and generates a self-contained HTML document based on it. Requires a 'system_plan' to be present in the state."

  # file_specialist:
  #   type: "llm"
  #   prompt_file: "file_specialist_prompt.md"
  #   description: "Performs file system operations like read, write, and list directories using a Pydantic schema. Write operations are protected by a safety lock."
  #   root_dir: "./workspace" # Defaults to a 'workspace' folder in the project root.

  # --- Data & Analysis ---
  data_extractor_specialist:
    type: "llm"
    prompt_file: "data_extractor_prompt.md" # Use a prompt designed for Pydantic/JSON schema output.
    description: "Extracts structured data from unstructured text based on a Pydantic schema. Requires 'text_to_process' in the state."

  sentiment_classifier_specialist:
    type: "llm"
    prompt_file: "sentiment_classifier_prompt.md"
    description: "Classifies the sentiment of a given text as positive, negative, or neutral."

  data_processor_specialist:
    type: "procedural"
    description: "A specialist that performs deterministic data processing tasks, like formatting or cleaning, without calling an LLM."

  text_analysis_specialist:
    type: "llm"
    prompt_file: "text_analysis_prompt.md"
    description: "Analyzes, summarizes, or extracts information from a block of text. Use this after text has been retrieved by another specialist."

  # --- General & Fallback ---
  prompt_specialist:
    type: "llm"
    prompt_file: "prompt_specialist_prompt.md"
    description: "A general-purpose specialist for direct Q&A and instruction following. Use this as a fallback when no other specialist seems appropriate for the user's request."

  # --- Wrapped & External Specialists ---
  open_interpreter_specialist:
    type: "wrapped_code"
    wrapper_path: "./external_agents/OpenInterpreter/open_interpreter_wrapper.py" # Path to the wrapper class file, relative to the project root.
    class_name: "OpenInterpreterAgent" # The name of the class to instantiate from the source file.
    description: "Executes shell commands and code (Python, etc.) to perform file system operations (list, read, write), data analysis, or web research. This is the primary tool for interacting with the local machine's files and running scripts."

  archiver_specialist:
    type: "procedural"
    archive_path: "./archives" # The directory to save final run reports.
    # Pruning strategy for the archive directory to prevent it from growing indefinitely.
    # Options: "none", "count", "size".
    pruning_strategy: "count"
    pruning_max_count: 50      # Max number of reports to keep if strategy is "count".
    # pruning_max_size_mb: 100 # Max total size in MB if strategy is "size".
    description: "Summarizes the conversation and prepares the final report. This is the final step."

  prompt_triage_specialist:
    type: "llm"
    prompt_file: "prompt_triage_prompt.md"
    description: "A specialist that performs a pre-flight check on the user's initial prompt for sentiment, coherence, and actionability."
